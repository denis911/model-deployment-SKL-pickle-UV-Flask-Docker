{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c21ca0-9d5f-4354-bc60-faac8d545b1e",
   "metadata": {},
   "source": [
    "## Imports and data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73289d35-11e7-45ed-89ea-69e93f96d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c7e824-be6d-4135-85cb-2ae986a437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about data preparation, where we need to read the csv file, \n",
    "# make the column names more homogenous, and deal with categorical and numerical values.\n",
    "\n",
    "# Data preparation\n",
    "data_url = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "\n",
    "df = pd.read_csv(data_url)\n",
    " \n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    " \n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    " \n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    " \n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    " \n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c57dfa-04fe-4169-9f8e-18da2f12efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our label - we will not use it explicitly, but in case...\n",
    "\n",
    "y_train = df.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05436278-d104-4f72-a098-5b1ae3fd4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about data splitting. Again we use the train_test_split \n",
    "# function to divide the dataset in full_train and test data.\n",
    "\n",
    "# Data splitting\n",
    " \n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f370a5-5b4c-49a8-862b-3962b4b44cdb",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9b19c6-2f12-4cc2-9a03-2a5023eb313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset - define the numerical column names and categorical column names.\n",
    "\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    " \n",
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
    "       'paymentmethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4397395-6490-456b-a6b3-859874d3854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about the train function. It has three arguments – the training dataframe \n",
    "# and the target values y_train, and the third argument is C which is a LogisticRegression parameter \n",
    "# for our model. First step here is to create dictionaries from the categorical columns, remember the \n",
    "# numerical columns are ignored here. Next we create a DictVectorizer instance which we need to use \n",
    "# fit_transform function on the dictionaries. So we get the X_train. Then we create our model which is \n",
    "# a logistic regression model, that we can use for training (fit function) based on the training data \n",
    "# (X_train and y_train). To apply the model later we need to return the DictVectorizer and the model as well.\n",
    "\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    " \n",
    "    model = LogisticRegression(C=C, max_iter=1000, solver='liblinear')\n",
    "    # If you don't add solver='liblinear' then model will complain with red during training...\n",
    "    model.fit(X_train, y_train)\n",
    " \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "339839a6-c901-4709-930e-1ac3087d6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I just mentioned in the paragraph before to use the model we need also the DictVectorizer. \n",
    "# Both are arguments for the predict function which is show in the next snippet. \n",
    "# Besides both arguments you also need a dataframe where we can provide a prediction for. \n",
    "# First step here is the same like in training function, we need to get the dictionaries. \n",
    "# This can be transformed by the DictVectorizer so we get the X, what we need to make a prediction on. \n",
    "# What we return here is the predicted probability for churning.\n",
    "\n",
    "def predict(df, dv, model):\n",
    "     dicts = df[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "     X = dv.transform(dicts)\n",
    "     y_pred = model.predict_proba(X)[:,1]\n",
    " \n",
    "     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8476975c-77e2-42ca-96dc-7abf8f60634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next snippet is to setup two parameters. The first one is the C value for the Logistic Regression model, \n",
    "# and the ‘n_splits’ parameter tells us how many splits we’re going to use in K-Fold cross-validation. \n",
    "# Here, we’re using 5 splits.\n",
    "\n",
    "C = 1.0\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f04e0c4-43c9-414e-a16d-85cf14b368ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.841 +- 0.007\n"
     ]
    }
   ],
   "source": [
    "# Next snippet shows the implemented K-Fold cross validation, where we use the parameters from the last snippet. \n",
    "# The for loop loops over all folds and does a training for each. \n",
    "# After that we calculate the roc_auc_score and collect the values for each fold. \n",
    "# At the end the mean score and the standard deviation for all folds are printed.\n",
    "\n",
    "# We use this step to confirm that our model is stable - e.g. it does not vary a lot\n",
    "# on different sets of data:\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)  \n",
    " \n",
    "scores = []\n",
    " \n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    " \n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    " \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    " \n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n",
    " \n",
    "# Output: C=1.0 0.841 +- 0.007 - very good/stable as st.dev is small..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa3cd6c0-7ba7-48f5-b117-7c577fa554b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8579400803839363"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last step is to train the final model based on the full_train data. \n",
    "# The steps here are similar to the steps mentioned before. \n",
    "# First is model training, then predicting the test data, and lastly calculate the roc_auc_score. \n",
    "# We see a value of 84% which is a bit higher than the average of the k-folds. \n",
    "# But there is not a big difference.\n",
    "\n",
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "y_test = df_test.churn.values\n",
    " \n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc\n",
    " \n",
    "# Output: 0.8579400803839363 - good - 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917eac4-1ae0-4242-a700-de81898e4ead",
   "metadata": {},
   "source": [
    "## Saving the model to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94ac148a-391f-4137-8604-c5b5547e4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to pickle\n",
    "# For saving the model we’ll use pickle, what is a built in library for saving Python objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ee363-05bd-4661-b2f0-28bfd076dd88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
