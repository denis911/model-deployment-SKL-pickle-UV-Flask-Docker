{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c21ca0-9d5f-4354-bc60-faac8d545b1e",
   "metadata": {},
   "source": [
    "## Imports and data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73289d35-11e7-45ed-89ea-69e93f96d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c7e824-be6d-4135-85cb-2ae986a437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about data preparation, where we need to read the csv file, \n",
    "# make the column names more homogenous, and deal with categorical and numerical values.\n",
    "\n",
    "# Data preparation\n",
    "data_url = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "\n",
    "df = pd.read_csv(data_url)\n",
    " \n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    " \n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    " \n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    " \n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    " \n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c57dfa-04fe-4169-9f8e-18da2f12efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our label - we will not use it explicitly, but in case...\n",
    "\n",
    "y_train = df.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05436278-d104-4f72-a098-5b1ae3fd4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about data splitting. Again we use the train_test_split \n",
    "# function to divide the dataset in full_train and test data.\n",
    "\n",
    "# Data splitting\n",
    " \n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f370a5-5b4c-49a8-862b-3962b4b44cdb",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd9b19c6-2f12-4cc2-9a03-2a5023eb313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset - define the numerical column names and categorical column names.\n",
    "\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    " \n",
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
    "       'paymentmethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4397395-6490-456b-a6b3-859874d3854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about the train function. It has three arguments – the training dataframe \n",
    "# and the target values y_train, and the third argument is C which is a LogisticRegression parameter \n",
    "# for our model. First step here is to create dictionaries from the categorical columns, remember the \n",
    "# numerical columns are ignored here. Next we create a DictVectorizer instance which we need to use \n",
    "# fit_transform function on the dictionaries. So we get the X_train. Then we create our model which is \n",
    "# a logistic regression model, that we can use for training (fit function) based on the training data \n",
    "# (X_train and y_train). To apply the model later we need to return the DictVectorizer and the model as well.\n",
    "\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    " \n",
    "    model = LogisticRegression(C=C, max_iter=1000, solver='liblinear')\n",
    "    # If you don't add solver='liblinear' then model will complain with red during training...\n",
    "    model.fit(X_train, y_train)\n",
    " \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339839a6-c901-4709-930e-1ac3087d6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As I just mentioned in the paragraph before to use the model we need also the DictVectorizer. \n",
    "# Both are arguments for the predict function which is show in the next snippet. \n",
    "# Besides both arguments you also need a dataframe where we can provide a prediction for. \n",
    "# First step here is the same like in training function, we need to get the dictionaries. \n",
    "# This can be transformed by the DictVectorizer so we get the X, what we need to make a prediction on. \n",
    "# What we return here is the predicted probability for churning.\n",
    "\n",
    "def predict(df, dv, model):\n",
    "     dicts = df[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "     X = dv.transform(dicts)\n",
    "     y_pred = model.predict_proba(X)[:,1]\n",
    " \n",
    "     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8476975c-77e2-42ca-96dc-7abf8f60634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next snippet is to setup two parameters. The first one is the C value for the Logistic Regression model, \n",
    "# and the ‘n_splits’ parameter tells us how many splits we’re going to use in K-Fold cross-validation. \n",
    "# Here, we’re using 5 splits.\n",
    "\n",
    "C = 1.0\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f04e0c4-43c9-414e-a16d-85cf14b368ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.841 +- 0.007\n"
     ]
    }
   ],
   "source": [
    "# Next snippet shows the implemented K-Fold cross validation, where we use the parameters from the last snippet. \n",
    "# The for loop loops over all folds and does a training for each. \n",
    "# After that we calculate the roc_auc_score and collect the values for each fold. \n",
    "# At the end the mean score and the standard deviation for all folds are printed.\n",
    "\n",
    "# We use this step to confirm that our model is stable - e.g. it does not vary a lot\n",
    "# on different sets of data:\n",
    "\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)  \n",
    " \n",
    "scores = []\n",
    " \n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    " \n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    " \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    " \n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n",
    " \n",
    "# Output: C=1.0 0.841 +- 0.007 - very good/stable as st.dev is small..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa3cd6c0-7ba7-48f5-b117-7c577fa554b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8579400803839363"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last step is to train the final model based on the full_train data. \n",
    "# The steps here are similar to the steps mentioned before. \n",
    "# First is model training, then predicting the test data, and lastly calculate the roc_auc_score. \n",
    "# We see a value of 84% which is a bit higher than the average of the k-folds. \n",
    "# But there is not a big difference.\n",
    "\n",
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "y_test = df_test.churn.values\n",
    " \n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc\n",
    " \n",
    "# Output: 0.8579400803839363 - good - 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0917eac4-1ae0-4242-a700-de81898e4ead",
   "metadata": {},
   "source": [
    "## Saving the model to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ac148a-391f-4137-8604-c5b5547e4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to pickle\n",
    "# For saving the model we’ll use pickle, what is a built in library for saving Python objects.\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468ee363-05bd-4661-b2f0-28bfd076dd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_C=1.0.bin'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we need to name our model file before we can write it to a file. \n",
    "# The following snippet demonstrates two ways of naming the file.\n",
    "\n",
    "output_file = 'model_C=%s.bin' % C\n",
    "output_file\n",
    "# Output: 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f92dd1-bc30-4706-8591-bd0719b55051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_C=1.0.bin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or alternatively - by using Python f string\n",
    "\n",
    "output_file = f'model_C={C}.bin'\n",
    "output_file\n",
    "# Output: 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1c3da62-f930-434a-82cc-3c393e3f5d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to create a file with that file name. ‘wb’ means Write Binary. \n",
    "# We need to save DictVectorizer and the model as well, because with just the model we’ll \n",
    "# not be able to translate a customer into a feature matrix. \n",
    "# Closing the file is crucial. Otherwise, we cannot be certain whether this file truly contains the content.\n",
    "\n",
    "# To avoid accidentally forgetting to close the file, we can use the ‘with’ statement, \n",
    "# which ensures that the file is closed automatically. Everything we do inside the ‘with’ \n",
    "# statement keeps the file open. However, once we exit this statement, the file is automatically closed.\n",
    "\n",
    "with open(output_file, 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389b46d-b809-41b5-9078-cc391fd80f59",
   "metadata": {},
   "source": [
    "## Loading the model with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4134818-b185-4d48-8057-3f9fa8dd9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model with Pickle\n",
    "# For loading the model we’ll also use pickle.\n",
    "\n",
    "import pickle\n",
    "\n",
    "model_file = 'model_C=1.0.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f038f67a-b970-4275-97c5-ceb370841098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DictVectorizer(sparse=False),\n",
       " LogisticRegression(max_iter=1000, solver='liblinear'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We also utilize the ‘with’ statement for loading the model. \n",
    "# Here, ‘rb’ denotes Read Binary. We employ the ‘load’ function from pickle, \n",
    "# which returns both the DictVectorizer and the model.\n",
    "\n",
    "with open(model_file, 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)\n",
    " \n",
    "dv, model\n",
    "# Output: (DictVectorizer(sparse=False), LogisticRegression(max_iter=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62bb44a5-967c-41b4-a409-e3932e13a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After loading the model, let’s use it to score one sample customer.\n",
    "\n",
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'month-to-month',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb29eec-7a12-4991-9ecb-a8e80371c2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "         0.  ,  1.  ,  0.  ,  0.  , 29.85,  0.  ,  1.  ,  0.  ,  0.  ,\n",
       "         0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,\n",
       "         0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "         0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  , 29.85]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before we can apply the predict function to this customer we need to turn it into a feature matrix. \n",
    "# The DictVectorizer expects a list of dictionaries, that’s why we create a list with one customer.\n",
    "\n",
    "X = dv.transform([customer])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87f7016f-37d7-42f7-b83c-2365dd4bee9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35669865, 0.64330135]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use predict function to get the probability that this particular customer is going to churn. \n",
    "# We’re interested in the second element, so we need to set the row=0 and column=1.\n",
    "\n",
    "model.predict_proba(X)\n",
    "# Output: array([[0.35669865, 0.64330135]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e34e1fe-c385-46b0-aeb8-8017bd4e25d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6433013495573104)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)[0,1]\n",
    "# Output: 0.6433013495573104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c836d923-f618-4655-9f1e-2b7141871365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X) # - 1 or churn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcadfa85-ccc3-4068-b703-37bf25cd9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning our notebook into a Python script\n",
    "# We can turn the Jupyter Notebook code into a Python file. \n",
    "# One easy way of doing this is click on “File” -> “Download as” and then “Python (.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56950582-b48e-4a65-a99f-2534e255b89e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
