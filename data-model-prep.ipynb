{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c21ca0-9d5f-4354-bc60-faac8d545b1e",
   "metadata": {},
   "source": [
    "## Imports and data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73289d35-11e7-45ed-89ea-69e93f96d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c7e824-be6d-4135-85cb-2ae986a437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about data preparation, where we need to read the csv file, \n",
    "# make the column names more homogenous, and deal with categorical and numerical values.\n",
    "\n",
    "# Data preparation\n",
    "data_url = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "\n",
    "df = pd.read_csv(data_url)\n",
    " \n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    " \n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    " \n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    " \n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    " \n",
    "df.churn = (df.churn == 'yes').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37c57dfa-04fe-4169-9f8e-18da2f12efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our label\n",
    "\n",
    "y_train = df.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05436278-d104-4f72-a098-5b1ae3fd4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about data splitting. Again we use the train_test_split \n",
    "# function to divide the dataset in full_train and test data.\n",
    "\n",
    "# Data splitting\n",
    " \n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f370a5-5b4c-49a8-862b-3962b4b44cdb",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9b19c6-2f12-4cc2-9a03-2a5023eb313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset - define the numerical column names and categorical column names.\n",
    "\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    " \n",
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
    "       'paymentmethod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4397395-6490-456b-a6b3-859874d3854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next snippet is about the train function. It has three arguments â€“ the training dataframe \n",
    "# and the target values y_train, and the third argument is C which is a LogisticRegression parameter \n",
    "# for our model. First step here is to create dictionaries from the categorical columns, remember the \n",
    "# numerical columns are ignored here. Next we create a DictVectorizer instance which we need to use \n",
    "# fit_transform function on the dictionaries. So we get the X_train. Then we create our model which is \n",
    "# a logistic regression model, that we can use for training (fit function) based on the training data \n",
    "# (X_train and y_train). To apply the model later we need to return the DictVectorizer and the model as well.\n",
    "\n",
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    " \n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    " \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339839a6-c901-4709-930e-1ac3087d6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "As I just mentioned in the paragraph before to use the model we need also the DictVectorizer. Both are arguments for the predict function which is show in the next snippet. Besides both arguments you also need a dataframe where we can provide a prediction for. First step here is the same like in training function, we need to get the dictionaries. This can be transformed by the DictVectorizer so we get the X, what we need to make a prediction on. What we return here is the predicted probability for churning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
